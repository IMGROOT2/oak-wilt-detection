{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = Path('../models')\n",
    "ENRICHED_CSV_PATH = DATA_DIR / 'oak_wilt_cluster_enriched.csv'\n",
    "\n",
    "# Ensure models dir exists\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(ENRICHED_CSV_PATH)\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"Feature Engineering...\")\n",
    "df['log_density'] = np.log1p(df['point_density'])\n",
    "df['log_duration'] = np.log1p(df['duration_days'])\n",
    "\n",
    "feature_cols = [\n",
    "    'log_duration',\n",
    "    'log_density',\n",
    "    'avg_precip',\n",
    "    'avg_humidity',\n",
    "    'avg_wind'\n",
    "]\n",
    "\n",
    "# Determine target column\n",
    "target_col = 'radius_ft' \n",
    "if target_col not in df.columns:\n",
    "    if 'radius_m' in df.columns:\n",
    "        target_col = 'radius_m'\n",
    "    else:\n",
    "        raise ValueError(\"Could not find radius target column (radius_ft or radius_m)\")\n",
    "        \n",
    "print(f\"Using target column: {target_col}\")\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# Handle NaNs\n",
    "print(\"Handling NaNs...\")\n",
    "# Combine X and y to drop rows where any are NaN\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "initial_len = len(data_combined)\n",
    "data_combined = data_combined.dropna()\n",
    "print(f\"Dropped {initial_len - len(data_combined)} rows with missing values.\")\n",
    "\n",
    "X = data_combined[feature_cols]\n",
    "y = data_combined[target_col]\n",
    "\n",
    "print(f\"Final dataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for Conformal Prediction (Train vs Calibration)\n",
    "# We reserve 20% of the data for calibration (Conformal Prediction)\n",
    "X_train, X_calib, y_train, y_calib = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Calibration Set: {len(X_calib)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Gradient Boosting Models...\")\n",
    "\n",
    "# 1. Lower Quantile (10th percentile) - Conservative Estimate\n",
    "model_lower = GradientBoostingRegressor(loss='quantile', alpha=0.1, n_estimators=100, random_state=42)\n",
    "model_lower.fit(X_train, y_train)\n",
    "\n",
    "# 2. Median (50th percentile) - Expected Spread\n",
    "model_median = GradientBoostingRegressor(loss='quantile', alpha=0.5, n_estimators=100, random_state=42)\n",
    "model_median.fit(X_train, y_train)\n",
    "\n",
    "# 3. Upper Quantile (90th percentile) - Severe Case\n",
    "model_upper = GradientBoostingRegressor(loss='quantile', alpha=0.9, n_estimators=100, random_state=42)\n",
    "model_upper.fit(X_train, y_train)\n",
    "\n",
    "print(\"Models trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad533f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Conformal Prediction Scores (CQR)...\")\n",
    "\n",
    "# Predict on Calibration Set\n",
    "pred_lower_calib = model_lower.predict(X_calib)\n",
    "pred_upper_calib = model_upper.predict(X_calib)\n",
    "\n",
    "# Calculate Non-conformity Scores\n",
    "# Score = max(lower - y, y - upper)\n",
    "# If y is inside [lower, upper], score is negative (how far inside)\n",
    "# If y is outside, score is positive (how far outside)\n",
    "scores = np.maximum(pred_lower_calib - y_calib, y_calib - pred_upper_calib)\n",
    "\n",
    "# Calculate q-value for 80% coverage (10th to 90th is 80% interval)\n",
    "# We want to cover 1 - alpha = 0.8\n",
    "alpha = 0.2 # 20% allowed error\n",
    "q_hat = np.quantile(scores, 1 - alpha)\n",
    "\n",
    "# Correction factor for Conformal Prediction\n",
    "# We will expand (or contract) the interval by q_hat\n",
    "print(f\"Conformal Correction Factor (q_hat): {q_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Calibration Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scores, bins=20, alpha=0.7, color='purple', label='Non-conformity Scores')\n",
    "plt.axvline(q_hat, color='red', linestyle='--', linewidth=2, label=f'q_hat ({q_hat:.2f})')\n",
    "plt.title('Calibration Scores Distribution')\n",
    "plt.xlabel('Score (Positive = Outside Interval)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15920ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models and q_hat\n",
    "print(\"Saving models...\")\n",
    "joblib.dump(model_lower, MODELS_DIR / 'gbr_lower.pkl')\n",
    "joblib.dump(model_median, MODELS_DIR / 'gbr_median.pkl')\n",
    "joblib.dump(model_upper, MODELS_DIR / 'gbr_upper.pkl')\n",
    "\n",
    "# Save q_hat\n",
    "joblib.dump(q_hat, MODELS_DIR / 'conformal_q.pkl')\n",
    "\n",
    "print(f\"Models saved to {MODELS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
